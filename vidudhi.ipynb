{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2caf685b-f5d9-490b-8fc5-333ee2b1f157",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio.transforms as T\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchaudio\n",
    "\n",
    "\n",
    "def preprocess_audio(waveform, sample_rate, num_frames=160):\n",
    "    # Resample if not already at 16000 Hz\n",
    "    if sample_rate != 16000:\n",
    "        resample_transform = T.Resample(orig_freq=sample_rate, new_freq=16000)\n",
    "        waveform = resample_transform(waveform)\n",
    "    \n",
    "    # Extract MFCC features\n",
    "    mfcc_transform = T.MFCC(sample_rate=16000, n_mfcc=13,\n",
    "                            melkwargs={'n_fft': 400, 'hop_length': 160, 'n_mels': 23, 'center': False})\n",
    "    mfcc = mfcc_transform(waveform)\n",
    "    \n",
    "    # Pad or truncate to fixed length\n",
    "    mfcc_length = mfcc.shape[2]\n",
    "    if mfcc_length > num_frames:\n",
    "        mfcc = mfcc[:, :, :num_frames]  # Truncate\n",
    "    elif mfcc_length < num_frames:\n",
    "        pad_size = num_frames - mfcc_length\n",
    "        mfcc = torch.nn.functional.pad(mfcc, (0, pad_size), \"constant\", 0)  # Pad with zeros\n",
    "    \n",
    "    return mfcc\n",
    "\n",
    "class VoiceDataset(Dataset):\n",
    "    def __init__(self, directory):\n",
    "        self.files = [os.path.join(root, file) for root, _, files in os.walk(directory) for file in files if file.endswith('.wav')]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        waveform, sample_rate = torchaudio.load(self.files[idx])\n",
    "        mfcc = preprocess_audio(waveform, sample_rate)\n",
    "        return mfcc, 1  # 1 for 'trigger word', 0 for 'no trigger word' or background noise\n",
    "\n",
    "# Now, your DataLoader should be able to batch the data without the size mismatch error.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1538894-11f2-42d2-8156-5eabf23d4064",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class TriggerWordModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TriggerWordModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=(3, 3), padding=1)  # Feature channels: 16, Output: [batch_size, 16, 13, 160]\n",
    "        self.pool = nn.MaxPool2d(2, 2)  # Output dimension: [batch_size, 16, 6, 80] (H and W halved)\n",
    "        self.lstm = nn.LSTM(6 * 80 * 16, 128, batch_first=True)  # Adjusted for the flattened output from conv + pool\n",
    "        self.fc = nn.Linear(128, 2)  # Expecting 128 features from LSTM\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.squeeze(2)  # Removing the extra dimension\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the output from the pooling layer\n",
    "        x = x.unsqueeze(1)  # Add sequence dimension for LSTM\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.fc(x[:, -1, :])  # Use the output of the last LSTM step\n",
    "        return torch.log_softmax(x, dim=1)\n",
    "\n",
    "model = TriggerWordModel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80380ad6-4f9b-4b6d-87fa-d5c130a48c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zy/m11q2_j93ml42v6cl0jqqz680000gn/T/ipykernel_30904/2555418368.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  loss = loss_function(outputs, torch.tensor(labels, dtype=torch.long))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 0.030325978994369507\n",
      "Epoch 2: Loss = 0.011222492903470993\n",
      "Epoch 3: Loss = 0.005913855973631144\n",
      "Epoch 4: Loss = 0.004091701935976744\n",
      "Epoch 5: Loss = 0.0031207927968353033\n",
      "Epoch 6: Loss = 0.0025056179147213697\n",
      "Epoch 7: Loss = 0.0020761380437761545\n",
      "Epoch 8: Loss = 0.0017584589077159762\n",
      "Epoch 9: Loss = 0.0015145983779802918\n",
      "Epoch 10: Loss = 0.0013222293928265572\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "from torch.nn import NLLLoss\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "loss_function = NLLLoss()\n",
    "\n",
    "def train_model(model, dataloader, epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for mfcc, labels in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(mfcc)\n",
    "            loss = loss_function(outputs, torch.tensor(labels, dtype=torch.long))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f'Epoch {epoch + 1}: Loss = {loss.item()}')\n",
    "\n",
    "# Example usage\n",
    "train_model(model, dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110854a1-7951-497a-9aae-a44f692b522b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
